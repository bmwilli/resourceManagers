## MAJOR ISSUES:
* Large timeout for Streams seems to cause really slow reaction to things like:
	* notify of revoke
	* <wait>
	* Requests release
	* <wait>
	* Streamtool exits that not enough resources could be allocated
	
* Mitigation: Add some delay between when task goes RUNNING and notifying streams
* Why? Mesos Command Executor returns running when shell sharts running, but if Streams is going to fail
* it will happen rather quickly in the case of pre-installed software, or it will take a long time if deploying
* if it happens quicly we can retry and thus avoid telling Streams we have allocated the resource



## Resource Specification Commands
### Show resource specifications
streamtool lsresources -l
### Remove resource specification
streamtool rmresourcespec 2
### Add resource specification
streamtool addresourcespec --numresources 1
streamtool addresourcespec --numresources 1[exclusive]
streamtool addresourcespec --numresources 1,ingest
streamtool addresourcespec --numresources 1,ingest[exclusive]

## Concern ISSUE!!
* Using mesos CommandExecutor
  - It may report running while command is still running streamsresourcesetup.sh and controller is not yet started
  - Only way to get around this may be to implement our own executor that does not report running until it is
  - Work around: Like YARN resource manager set controller.startTimeout=300
* Streams system/impl/bin/startDomainController.sh has a which java before running dependency checker
  - On compute nodes without their own java, this reports error and does not run dependency checker
  - Need to report to IBM
  - Minor becuase later in the script it runs the domain controller using Streams internal java install
  
  

## To Do (Working notes to get version 0.1.0 ready)
		
* Handle error condition when framework killed but zookeeper thinks domain is running
  * streamtool stopdomain --force will try and release resource...handle gracefully
* Initial version of --deploy (done)
  * Need test on HDFS
* Validate Stop???
  * Prevent shutdown (unless --force) if we have resources being used

## Development issues:
* Started using the YARN resource manager as a model where only handled specified domain and zookeeper
* Moving toward Symphony approach to support multiple
* Code in transition (example allocateResources, get domainID from clientInfo or argument?)
	* For now do both and look for inconsistencies
	* Future move to just cilentID and remove domainId as a required argument


## Future
* Utilize resource requirements in the requests to limit the offers from Mesos to what we need
	* Cuts down on chatter and offers from Mesos
* Handle multiple multiple clients (e.g. multiple domains)
* Instructions and test for PKI authentication
* Marathon submission of framework
  * run streams-on-mesos from marathon
* Web interface to get internal state
* Persistent State manager - see Symphony Resource Manager
	* ValidateState
* High Availability
	* Mesos - Handle Master failover, Reconcile Tasks, Handle Scheduler failover
	* Streams - Do the same
* Convert exceptions to ResourceManagerMessageException, did not see the API for that
* Enhance Exceptions to use Locale (see Symphony)
* Handle rolling upgrade scenarios and starting isntances at different versions.  Streams version is part of request for resources.  Generate multiple versions of resource packages to send out
* Resource packing when offers have more resources that needed (see Building applications on mesos book)
  * Do not recommend doing this becuase will overload a single host perhaps

## Features / Issues Handled
* Implemented WAIT_ALLOCATED_SECS default of 2 to wait for a task to be running for that long before notifying streams
	* This prevents notification of allocated and then revoking resource if it fails quickly
	* this is the kind of failure that occurs if you have pre-installed streams, however, one of the slaves is not valid
	  and the start of the controller fails.  The mesos task reports running for a brief amount of time then reports failed
	* By waiting for it to be running for 2 seconds if it fails within that time, we simply try again when a new offer 
	  is available.  In testing it could take many, many attempts, but it usually finally gets an offer from a good slave
* Supress Mesos Offers when there are no outstanding resource requests
	* Only when we have a resource request that needs resources do we tell Mesos to send us offers
* Streams Resource Manager custom commands to get internal state of allResources list, etc.



## Testing on Linux Workstation
I have not tested these instructions yet

1. Build and Install Mesos (I used 1.0.1)

	See Mesos instructions

	export LD_LIBRARY_PATH="$LD_LIBRARY_PATH:/usr/local/lib"

2. Run standalone Mesos and Slave (no zookeeper)

	mesos-master --ip=127.0.0.1 --work_dir=/var/lib/mesos
	mesos-agent --master=127.0.0.1:5050 --work_dir=/var/lib/mesos


3. Run zookeeper (from docker) (for Streams).

	docker run -d \
	-p 2181:2181 \
	-p 2888:2888 \
	-p 3888:3888 \
	jplock/zookeeper

	export STERAMS_ZKCONNECT=localhost:2181

4. Run streams-on-mesos using standalone master uri

	cd scripts
	./streams-on-mesos start --master localhost:5050 --deploy

	
	
## Testing on Workstation with Docker for development


0. Get IP Address of Docker Server

	export HOST_IP=127.0.0.1

1. Run zookeeper.

	docker run -d \
	> -p 2181:2181 \
	> -p 2888:2888 \
	> -p 3888:3888 \
	> garland/zookeeper

	export STERAMS_ZKCONNECT=localhost:2181

2. Ensure Streams variables set

	source <streams install location>/4.2.0.0/bin/streamsprofile.sh

3. Start Mesos Master

	docker run --net="host" \
	-p 5050:5050 \
	-e "MESOS_HOSTNAME=${HOST_IP}" \
	-e "MESOS_IP=${HOST_IP}" \
	-e "MESOS_ZK=zk://${HOST_IP}:2181/mesos" \
	-e "MESOS_PORT=5050" \
	-e "MESOS_LOG_DIR=/var/log/mesos" \
	-e "MESOS_QUORUM=1" \
	-e "MESOS_REGISTRY=in_memory" \
	-e "MESOS_WORK_DIR=/var/lib/mesos" \
	-d \
	garland/mesosphere-docker-mesos-master
	
	
## Development Questions:
* sampleRM had a lot of logic about re-using resources that are already beeing used but were not excluded by the new allocateResources request.  Should we implement that?  When would you do that?  Would we do that in the case that we configured the resource manager to take entire offers?
	* ANSWER Not required since we use containers.  Only required when giving out whole machines
	* Could come into play if we truly accepted entire offer